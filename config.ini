[DEFAULT]
required_packages = transformers,datasets,torch,huggingface_hub,termcolor,accelerate
output_dir = mini_models
mini_model_name = Mim-LLama
dataset_size = 1000
max_sequence_length = 512
num_classes = 8
model_name = distilbert-base-uncased
num_train_epochs = 3
per_device_train_batch_size = 16
per_device_eval_batch_size = 64
evaluation_strategy = epoch
save_strategy = epoch
learning_rate = 1e-5
save_total_limit = 2
load_best_model_at_end = true
metric_for_best_model = accuracy
greater_is_better = true
